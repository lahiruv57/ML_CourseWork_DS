{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training data:\n",
      " date_id            0\n",
      "item_dept          0\n",
      "item_qty           0\n",
      "net_sales          0\n",
      "store              0\n",
      "item               0\n",
      "invoice_num    22810\n",
      "dtype: int64\n",
      "Missing values in test data:\n",
      " date_id           0\n",
      "item_dept         0\n",
      "item_qty          0\n",
      "net_sales         0\n",
      "store             0\n",
      "item              0\n",
      "invoice_num    8305\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_7736\\1350996061.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  training_data['item_qty'].fillna(0, inplace=True)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_7736\\1350996061.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_data['item_qty'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows in training data: 1766\n",
      "Duplicate rows in test data: 750\n",
      "Data Cleaning Summary:\n",
      "- Missing values handled (filled with 0 or dropped).\n",
      "- Duplicate rows removed.\n",
      "- Date columns converted to datetime format.\n",
      "- Cleaned data saved for further use.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set display options for better visibility\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Paths to the data files\n",
    "outlet_info_path = '../data/outlet_info.csv'\n",
    "training_data_path = '../data/training_data.csv'\n",
    "test_data_path = '../data/test_data.csv'\n",
    "\n",
    "# Load the datasets (assuming load_data function is correctly implemented in src.utils.data_loader)\n",
    "outlet_info = pd.read_csv(outlet_info_path)\n",
    "training_data = pd.read_csv(training_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in training data:\\n\", training_data.isnull().sum())\n",
    "print(\"Missing values in test data:\\n\", test_data.isnull().sum())\n",
    "\n",
    "# Fill missing values\n",
    "training_data['item_qty'].fillna(0, inplace=True)\n",
    "test_data['item_qty'].fillna(0, inplace=True)\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Duplicate rows in training data: {training_data.duplicated().sum()}\")\n",
    "print(f\"Duplicate rows in test data: {test_data.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates\n",
    "training_data.drop_duplicates(inplace=True)\n",
    "test_data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "training_data['date_id'] = pd.to_datetime(training_data['date_id'])\n",
    "test_data['date_id'] = pd.to_datetime(test_data['date_id'])\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = '/mnt/data'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the cleaned datasets to new CSV files\n",
    "training_data.to_csv(os.path.join(output_dir, 'cleaned_training_data.csv'), index=False)\n",
    "test_data.to_csv(os.path.join(output_dir, 'cleaned_test_data.csv'), index=False)\n",
    "\n",
    "# Summary of Data Cleaning\n",
    "print(\"Data Cleaning Summary:\")\n",
    "print(\"- Missing values handled (filled with 0 or dropped).\")\n",
    "print(\"- Duplicate rows removed.\")\n",
    "print(\"- Date columns converted to datetime format.\")\n",
    "print(\"- Cleaned data saved for further use.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  store   profile    size\n",
      "0   ABC  Moderate  Medium\n",
      "1   XYZ      High   Large\n",
      "     date_id  item_dept  item_qty  net_sales store    item  invoice_num\n",
      "0  11/1/2021    Grocery       1.0      160.0   XYZ   16620    1475459.0\n",
      "1  11/1/2021    Grocery       2.0      480.0   XYZ   32365    1475459.0\n",
      "2  11/1/2021    Grocery       1.0      127.0   XYZ   31349    1475459.0\n",
      "3  11/1/2021  Household       2.0      110.0   XYZ    1266    1475475.0\n",
      "4  11/1/2021  Household       1.0      150.0   XYZ  114920    1475475.0\n",
      "    date_id  item_dept  item_qty  net_sales store    item  invoice_num\n",
      "0  2/1/2022  Beverages       2.0      480.0   XYZ  112360    1495518.0\n",
      "1  2/1/2022  Beverages       1.0      202.0   XYZ  111195    1495518.0\n",
      "2  2/1/2022  Household       1.0      165.0   XYZ   41212    1495572.0\n",
      "3  2/1/2022  Household       2.0      480.0   XYZ  123476    1495572.0\n",
      "4  2/1/2022    Grocery       2.0      660.0   XYZ  106668    1495572.0\n",
      "Missing values in training data:\n",
      " date_id            0\n",
      "item_dept          0\n",
      "item_qty           0\n",
      "net_sales          0\n",
      "store              0\n",
      "item               0\n",
      "invoice_num    22810\n",
      "dtype: int64\n",
      "Missing values in test data:\n",
      " date_id           0\n",
      "item_dept         0\n",
      "item_qty          0\n",
      "net_sales         0\n",
      "store             0\n",
      "item              0\n",
      "invoice_num    8305\n",
      "dtype: int64\n",
      "Missing values after cleaning:\n",
      " date_id            0\n",
      "item_dept          0\n",
      "item_qty           0\n",
      "net_sales          0\n",
      "store              0\n",
      "item               0\n",
      "invoice_num    22810\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_7736\\2961943720.py:41: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  training_data['item_qty'].fillna(0, inplace=True)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_7736\\2961943720.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_data['item_qty'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows in training data: 1766\n",
      "Duplicate rows in test data: 750\n",
      "Duplicate rows after cleaning in training data: 0\n",
      "Duplicate rows after cleaning in test data: 0\n",
      "date_id        datetime64[ns]\n",
      "item_dept              object\n",
      "item_qty              float64\n",
      "net_sales             float64\n",
      "store                  object\n",
      "item                    int64\n",
      "invoice_num           float64\n",
      "dtype: object\n",
      "date_id        datetime64[ns]\n",
      "item_dept              object\n",
      "item_qty              float64\n",
      "net_sales             float64\n",
      "store                  object\n",
      "item                    int64\n",
      "invoice_num           float64\n",
      "dtype: object\n",
      "Data Cleaning Summary:\n",
      "- Missing values handled (filled with 0 or dropped).\n",
      "- Duplicate rows removed.\n",
      "- Date columns converted to datetime format.\n",
      "- Cleaned data saved for further use.\n"
     ]
    }
   ],
   "source": [
    "# data_cleaning.ipynb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the project root directory (one level up from the current directory)\n",
    "project_root = os.path.abspath(\"..\")\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from src.utils.data_loader import load_data\n",
    "from src.utils.data_preprocessing import clean_data\n",
    "\n",
    "\n",
    "\n",
    "# Set display options for better visibility\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Paths to the data files\n",
    "outlet_info_path = '../data/outlet_info.csv'\n",
    "training_data_path = '../data/training_data.csv'\n",
    "test_data_path = '../data/test_data.csv'\n",
    "\n",
    "# Load the datasets\n",
    "outlet_info, training_data, test_data = load_data(outlet_info_path, training_data_path, test_data_path)\n",
    "\n",
    "# Display the first few rows of each dataset\n",
    "print(outlet_info.head())\n",
    "print(training_data.head())\n",
    "print(test_data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in training data:\\n\", training_data.isnull().sum())\n",
    "print(\"Missing values in test data:\\n\", test_data.isnull().sum())\n",
    "\n",
    "# Fill missing values\n",
    "training_data['item_qty'].fillna(0, inplace=True)\n",
    "test_data['item_qty'].fillna(0, inplace=True)\n",
    "\n",
    "# Optionally, drop rows with missing values in certain columns if they are critical\n",
    "# training_data.dropna(subset=['some_column'], inplace=True)\n",
    "# test_data.dropna(subset=['some_column'], inplace=True)\n",
    "\n",
    "# Verify that there are no more missing values\n",
    "print(\"Missing values after cleaning:\\n\", training_data.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Duplicate rows in training data: {training_data.duplicated().sum()}\")\n",
    "print(f\"Duplicate rows in test data: {test_data.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates\n",
    "training_data.drop_duplicates(inplace=True)\n",
    "test_data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Verify that duplicates are removed\n",
    "print(f\"Duplicate rows after cleaning in training data: {training_data.duplicated().sum()}\")\n",
    "print(f\"Duplicate rows after cleaning in test data: {test_data.duplicated().sum()}\")\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "training_data['date_id'] = pd.to_datetime(training_data['date_id'])\n",
    "test_data['date_id'] = pd.to_datetime(test_data['date_id'])\n",
    "\n",
    "# Verify the conversion\n",
    "print(training_data.dtypes)\n",
    "print(test_data.dtypes)\n",
    "\n",
    "# Save the cleaned datasets to new CSV files\n",
    "training_data.to_csv('/mnt/data/cleaned_training_data.csv', index=False)\n",
    "test_data.to_csv('/mnt/data/cleaned_test_data.csv', index=False)\n",
    "\n",
    "# Summary of Data Cleaning\n",
    "print(\"Data Cleaning Summary:\")\n",
    "print(\"- Missing values handled (filled with 0 or dropped).\")\n",
    "print(\"- Duplicate rows removed.\")\n",
    "print(\"- Date columns converted to datetime format.\")\n",
    "print(\"- Cleaned data saved for further use.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
